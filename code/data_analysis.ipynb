{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5099ad5a",
   "metadata": {},
   "source": [
    "# Data Analysis \n",
    "MRBench_V1: The original dataset with 192 dialogues as deatiled in the paper.\n",
    "\n",
    "MRBench_V2: An updated version with additional 8 dialogues, bringing the total to 200 examples.\n",
    "\n",
    "\n",
    "conduct DAMR / Annotation correlation (AC) scores for original result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "54c77316",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "454813c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_MRBench_response(data, model_name):\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     for i, dialogue in enumerate(data):\n",
    "#         print(\"*\"*100)\n",
    "#         print(f\"Dialogue {i+1}:\")\n",
    "#         print(f\"  Data: {dialogue['Data']}\")\n",
    "#         print(f\"  Topic: {dialogue['Topic']}\")\n",
    "#         # print(f\"  Conversation History: {dialogue['conversation_history']}\")\n",
    "#         print(f\"  >>>>Ground Truth Solution: {dialogue['Ground_Truth_Solution']}\")\n",
    "#         llm_model_response = dialogue['anno_llm_responses'][model_name]['response']\n",
    "#         print(f\"  >>>>LLM Response: {llm_model_response}\")\n",
    "#         print(\"*\"*100)\n",
    "\n",
    "# def print_MRBench_label(data, model_name, annotation_name):\n",
    "#     print(f\"Model: {model_name}\")\n",
    "#     from collections import defaultdict\n",
    "#     label_dict = defaultdict(int)\n",
    "#     for i, dialogue in enumerate(data):\n",
    "#         print(\"*\"*100)\n",
    "#         print(f\"Dialogue {i+1}:\")\n",
    "#         print(f\"  Data: {dialogue['Data']}\")\n",
    "#         # print(f\"  Topic: {dialogue['Topic']}\")\n",
    "#         # print(f\"  Conversation History: {dialogue['conversation_history']}\")\n",
    "#         # print(f\"  >>>>Ground Truth Solution: {dialogue['Ground_Truth_Solution']}\")\n",
    "#         annotation = dialogue['anno_llm_responses'][model_name]['annotation'][annotation_name]\n",
    "#         label_dict[annotation] += 1\n",
    "#         print(f\"  >>>>{annotation_name}: {annotation}\")\n",
    "#     print(label_dict)\n",
    "#     # label_dict  Yes / All\n",
    "#     for k, v in label_dict.items():\n",
    "#         print(f\"{k}: {v/len(data)*100}%\")\n",
    "# # print_MRBench_data(MRBenchv1_data, \"Mistral\")\n",
    "# # print_MRBench_label(MRBenchv1_data, \"Llama318B\", \"Revealing_of_the_Answer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d5227024",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_annotation_label(key, label):\n",
    "    label = label.lower().strip()\n",
    "    Tutor_tone_mapping = {\n",
    "        \"encouraging\": 1,\n",
    "        \"neutral\": 2,\n",
    "        \"offensive\": 3\n",
    "    }\n",
    "    \n",
    "    Other_rule_mapping = {\n",
    "        \"yes\": 1,\n",
    "        \"to some extent\": 2,\n",
    "        \"no\": 3\n",
    "    }\n",
    "    def map_revealing_of_the_answer(label):\n",
    "        label = label.lower().strip()\n",
    "        if label.startswith(\"yes\") and \"correct\" in label:\n",
    "            return 1\n",
    "        elif label.startswith(\"yes\") and \"incorrect\" in label:\n",
    "            return 2\n",
    "        elif label.startswith(\"no\"):\n",
    "            return 3\n",
    "        else:\n",
    "            return None\n",
    "    if key == \"Revealing_of_the_Answer\":\n",
    "        return map_revealing_of_the_answer(label)\n",
    "    else:\n",
    "        map_dict = Tutor_tone_mapping if key == \"Tutor_Tone\" else Other_rule_mapping\n",
    "        for key, value in map_dict.items():\n",
    "            if label.startswith(key):\n",
    "                return value\n",
    "        print(label)\n",
    "        return None\n",
    "\n",
    "\n",
    "desiderata = {\n",
    "    \"Mistake_Identification\": 1,  # Yes\n",
    "    \"Mistake_Location\": 1,        # Yes\n",
    "    \"Revealing_of_the_Answer\": 3,        # No\n",
    "    \"Providing_Guidance\": 1,      # Yes\n",
    "    \"Actionability\": 1,           # Yes\n",
    "    \"Coherence\": 1,                # Yes\n",
    "    \"Tutor_Tone\": 1,              # Encouraging\n",
    "    \"humanlikeness\": 1,               # Yes\n",
    "}\n",
    "\n",
    "# new_annotation\n",
    "def evaluate_ordinary_desiderata(data, data_type=\"All\", verbose=False):\n",
    "    from collections import defaultdict\n",
    "    evaluation_result = defaultdict(dict)\n",
    "    for data in MRBenchv1_data:\n",
    "        d_type= data['Data']\n",
    "        if data_type !=\"All\" and data_type != d_type:\n",
    "            print(f\"Skip {d_type}\")\n",
    "            continue\n",
    "        for model, value in data['anno_llm_responses'].items():\n",
    "            annotation_point = value['annotation_point']\n",
    "            for k, v in annotation_point.items(): \n",
    "                if v is None:\n",
    "                    if verbose:\n",
    "                        print(model, k, v)\n",
    "                    continue\n",
    "                if v == desiderata[k]:\n",
    "                    if k not in evaluation_result[model]:\n",
    "                        evaluation_result[model][k] = [0,0]\n",
    "                    evaluation_result[model][k][0] += 1\n",
    "                else:\n",
    "                    if k not in evaluation_result[model] and v is not None:\n",
    "                        evaluation_result[model][k] = [0,0]\n",
    "                    evaluation_result[model][k][1] += 1\n",
    "    return evaluation_result\n",
    "def print_evaluation_result(evaluation_result):\n",
    "    import pandas as pd\n",
    "    pd_result = []\n",
    "    columns = []\n",
    "    for model, value in evaluation_result.items():\n",
    "        model_result = []\n",
    "        value = sorted(value.items(), key=lambda x: x[0])\n",
    "        columns = [k for k, v in value]\n",
    "        for k, v in value:\n",
    "            model_result.append((v[0]/(v[0]+v[1] )* 100.0))\n",
    "        pd_result.append([model] + model_result)  \n",
    "    columns = ['Tutor'] + columns\n",
    "    pd_result = pd.DataFrame(pd_result, columns=columns)\n",
    "    columns_mapping = {\n",
    "        'Mistake_Identification': 'Mistake_Identification',\n",
    "        'Mistake_Location': 'Mistake_Location',\n",
    "        'Revealing_of_the_Answer': 'Revealing_of_the_Answer',\n",
    "        'Providing_Guidance': 'Providing_Guidance',\n",
    "        'Actionability': 'Actionability',\n",
    "        'Coherence': 'Coherence',\n",
    "        'Tutor_Tone': 'Tutor_Tone',\n",
    "        'humanlikeness': 'Human-likeness'\n",
    "    }\n",
    "    pd_result.rename(columns=columns_mapping, inplace=True)\n",
    "    pd_result = pd_result[['Tutor', 'Mistake_Identification', 'Mistake_Location', 'Revealing_of_the_Answer', 'Providing_Guidance', 'Actionability', 'Coherence', 'Tutor_Tone', 'Human-likeness']].round(2)\n",
    "    return pd_result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ecddba",
   "metadata": {},
   "source": [
    "# Data Analysis \n",
    "MRBench_V1: The original dataset with 192 dialogues as deatiled in the paper.\n",
    "\n",
    "MRBench_V2: An updated version with additional 8 dialogues, bringing the total to 200 examples.\n",
    "\n",
    "\n",
    "conduct DAMR score for original result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8e8bb0b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6f2b5b86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of dialogues in MRBenchv1 is  192\n",
      "Number of dialogues in MRBenchv2 is  200\n",
      "The maximum length of the ground truth solution in MRBenchv1 is  579\n",
      "MRBenchv1 - The number of dialogues annotated by  Gemini is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Phi3 is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama318B is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama31405B is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Mistral is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Expert is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  GPT4 is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Sonnet is  192\n",
      "MRBenchv1 - The number of dialogues annotated by  Novice is  53\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MRBenchv1 - The number of dialogues annotated by  Gemini_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Phi3_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama318B_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama31405B_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Mistral_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Expert_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  GPT4_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Sonnet_MathDial is  139\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama31405B_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Mistral_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  GPT4_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Llama318B_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Novice_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Phi3_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Expert_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Sonnet_Bridge is  53\n",
      "MRBenchv1 - The number of dialogues annotated by  Gemini_Bridge is  53\n",
      "----------------------------------------------------------------------------------------------------\n",
      "MRBenchv2 - The number of dialogues annotated by  Gemini is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Phi3 is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Llama318B is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Llama31405B is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Mistral is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Expert is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  GPT4 is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Sonnet is  200\n",
      "MRBenchv2 - The number of dialogues annotated by  Novice is  55\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "root_dir = \"../data\"\n",
    "MRBenchv1 = os.path.join(root_dir, \"MRBench/MRBench_V1.json\")\n",
    "MRBenchv2 = os.path.join(root_dir, \"MRBench/MRBench_V2.json\")\n",
    "MRBenchv1_data = json.load(open(MRBenchv1))\n",
    "MRBenchv2_data = json.load(open(MRBenchv2))\n",
    "print(\"Number of dialogues in MRBenchv1 is \", len(MRBenchv1_data))\n",
    "print(\"Number of dialogues in MRBenchv2 is \", len(MRBenchv2_data))\n",
    "# MRBenchv1_data[0]\n",
    "# count the length of the ground truth solution\n",
    "length_list = [len(data['Ground_Truth_Solution']) for data in MRBenchv1_data]\n",
    "print(\"The maximum length of the ground truth solution in MRBenchv1 is \", max(length_list))\n",
    "\n",
    "model_count = defaultdict(int)\n",
    "model_data_count = defaultdict(int)\n",
    "for data in MRBenchv1_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        model_count[key] += 1\n",
    "        model_data_count[key + \"_\" + data['Data']] += 1\n",
    "for k, v in model_count.items():\n",
    "    print(\"MRBenchv1 - The number of dialogues annotated by \", k, \"is \", v)\n",
    "\n",
    "print(\"-\"*100)\n",
    "for k, v in model_data_count.items():\n",
    "    print(\"MRBenchv1 - The number of dialogues annotated by \", k, \"is \", v)\n",
    "print(\"-\"*100)\n",
    "model_count = defaultdict(int)\n",
    "for data in MRBenchv2_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        model_count[key] += 1\n",
    "for k, v in model_count.items():\n",
    "    print(\"MRBenchv2 - The number of dialogues annotated by \", k, \"is \", v)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be341f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'conversation_id': '930-b01cb51d-748d-460c-841a-08e4d5cd5cc7', 'conversation_history': 'Tutor: Hi, could you please provide a step-by-step solution for the question below? The question is: Elliott is trying to walk 10,000 steps a day. He finished half of his steps on his walks to and from school and did another 1,000 steps going for a short walk with his friend. He also went for a short jog around the block and realized that after he had finished his jog, he only had 2,000 steps left to take. How many steps did Elliott take during his jog?\\xa0\\n\\xa0Student: Elliott finished half of his steps on his walks to and from school, so he took 10,000/2 = 5000 steps during these walks.\\nAdding the 1,000 steps he took with his friend, he has taken 5000+1000 = 6000 steps.\\nSubtracting 6000 from his goal of 10,000, he has 10,000-6000 = 4000 steps left to take.\\nTherefore, he took 4000 steps during his jog.\\xa0\\n\\xa04000\\xa0\\n\\xa0Tutor: can you tell me how you got to your answer?\\xa0\\n\\xa0Student: Sure. I started by figuring out how many steps Elliott had taken before his jog. I knew he had finished half of his steps on his walks to and from school so I calculated that he had taken 10000/2 = 5000 steps during these walks. Then I added the 1000 steps he took with his friend so I had taken 5000+1000 = 6000 steps. After that I subtracted 6000 from his goal of 10000 so I had 10000-6000 = 4000 steps left to take. This meant that he had taken 4000 steps during his jog.', 'Data': 'MathDial', 'Split': 'test', 'Topic': 'Not Available', 'Ground_Truth_Solution': 'Elliott took half of his steps on his school journey, which was 10,000 steps / 2 = 5,000 steps.\\nAdding this to his short walk means he has taken 5,000 + 1,000 = 6,000 steps.\\nHe only has 2,000 steps left, which means that during the day, he has taken 10,000 goal steps – 2,000 remaining steps = 8,000 steps.\\nThe difference between these is therefore the steps he took during his jog, which was 8,000 total steps – 6,000 steps on his school walks and short walks = 2,000 steps.\\n 2000', 'anno_llm_responses': {'Gemini': {'response': \"It looks like you're on the right track, but remember that Elliott still had 2,000 steps left to take *after* his jog.\", 'annotation': {'Mistake_Identification': 'Yes', 'Mistake_Location': 'Yes', 'Revealing_of_the_Answer': 'No', 'Providing_Guidance': 'Yes', 'Actionability': 'Yes', 'humanlikeness': 'Yes', 'Coherence': 'Yes', 'Tutor_Tone': 'Neutral'}, 'annotation_point': {'Mistake_Identification': 1, 'Mistake_Location': 1, 'Revealing_of_the_Answer': 3, 'Providing_Guidance': 1, 'Actionability': 1, 'humanlikeness': 1, 'Coherence': 1, 'Tutor_Tone': 2}}, 'Phi3': {'response': \"Great job! To further enhance your understanding, let's explore more examples and practice solving similar problems together.\", 'annotation': {'Mistake_Identification': 'No', 'Mistake_Location': 'No', 'Revealing_of_the_Answer': 'No', 'Providing_Guidance': 'No', 'Actionability': 'No', 'humanlikeness': 'Yes', 'Coherence': 'No', 'Tutor_Tone': 'Encouraging'}, 'annotation_point': {'Mistake_Identification': 3, 'Mistake_Location': 3, 'Revealing_of_the_Answer': 3, 'Providing_Guidance': 3, 'Actionability': 3, 'humanlikeness': 1, 'Coherence': 3, 'Tutor_Tone': 1}}, 'Llama318B': {'response': \"That's a good attempt, but I'm not seeing how you factored in the remaining 2000 steps Elliott had left after his jog into your calculation - could you show me how you think those steps fit into the equation?\", 'annotation': {'Mistake_Identification': 'Yes', 'Mistake_Location': 'Yes', 'Revealing_of_the_Answer': 'No', 'Providing_Guidance': 'Yes', 'Actionability': 'Yes', 'humanlikeness': 'Yes', 'Coherence': 'Yes', 'Tutor_Tone': 'Encouraging'}, 'annotation_point': {'Mistake_Identification': 1, 'Mistake_Location': 1, 'Revealing_of_the_Answer': 3, 'Providing_Guidance': 1, 'Actionability': 1, 'humanlikeness': 1, 'Coherence': 1, 'Tutor_Tone': 1}}, 'Llama31405B': {'response': 'Here\\'s a tutor response that acknowledges the student\\'s effort and encourages them to revisit their thinking:\\n\\n\"I see that you\\'ve carefully calculated the steps Elliott took before his jog, but I\\'m wondering if you considered the fact that he only had 2,000 steps left to take after his jog, which might affect the number of steps he took during the jog itself?\"', 'annotation': {'Mistake_Identification': 'Yes', 'Mistake_Location': 'Yes', 'Revealing_of_the_Answer': 'No', 'Providing_Guidance': 'Yes', 'Actionability': 'Yes', 'humanlikeness': 'No', 'Coherence': 'Yes', 'Tutor_Tone': 'Neutral'}, 'annotation_point': {'Mistake_Identification': 1, 'Mistake_Location': 1, 'Revealing_of_the_Answer': 3, 'Providing_Guidance': 1, 'Actionability': 1, 'humanlikeness': 3, 'Coherence': 1, 'Tutor_Tone': 2}}, 'Mistral': {'response': 'It seems like you forgot to consider the 2,000 steps Elliott still had left after his jog.', 'annotation': {'Mistake_Identification': 'Yes', 'Mistake_Location': 'Yes', 'Revealing_of_the_Answer': 'No', 'Providing_Guidance': 'Yes', 'Actionability': 'Yes', 'humanlikeness': 'Yes', 'Coherence': 'Yes', 'Tutor_Tone': 'Neutral'}, 'annotation_point': {'Mistake_Identification': 1, 'Mistake_Location': 1, 'Revealing_of_the_Answer': 3, 'Providing_Guidance': 1, 'Actionability': 1, 'humanlikeness': 1, 'Coherence': 1, 'Tutor_Tone': 2}}, 'Expert': {'response': 'But after his jog he still had 2000 steps left', 'annotation': {'Mistake_Identification': 'Yes', 'Mistake_Location': 'Yes', 'Revealing_of_the_Answer': 'No', 'Providing_Guidance': 'Yes', 'Actionability': 'Yes', 'humanlikeness': 'Yes', 'Coherence': 'Yes', 'Tutor_Tone': 'Neutral'}, 'annotation_point': {'Mistake_Identification': 1, 'Mistake_Location': 1, 'Revealing_of_the_Answer': 3, 'Providing_Guidance': 1, 'Actionability': 1, 'humanlikeness': 1, 'Coherence': 1, 'Tutor_Tone': 2}}, 'GPT4': {'response': \"I see where your confusion is. You subtracted the total steps he had already taken from his goal, but the problem states that after his jog, Elliott had 2,000 steps left, which means he must have done 2,000 steps during his jog, not 4,000. Let's try to recalculate it together.\", 'annotation': {'Mistake_Identification': 'Yes', 'Mistake_Location': 'Yes', 'Revealing_of_the_Answer': 'Yes (and the answer is correct)', 'Providing_Guidance': 'Yes', 'Actionability': 'No', 'humanlikeness': 'Yes', 'Coherence': 'Yes', 'Tutor_Tone': 'Encouraging'}, 'annotation_point': {'Mistake_Identification': 1, 'Mistake_Location': 1, 'Revealing_of_the_Answer': 1, 'Providing_Guidance': 1, 'Actionability': 3, 'humanlikeness': 1, 'Coherence': 1, 'Tutor_Tone': 1}}, 'Sonnet': {'response': \"Great job showing your work, but let's take another look at the last part of the problem statement to make sure we're considering all the information given.\", 'annotation': {'Mistake_Identification': 'To some extent', 'Mistake_Location': 'No', 'Revealing_of_the_Answer': 'No', 'Providing_Guidance': 'To some extent', 'Actionability': 'To some extent', 'humanlikeness': 'Yes', 'Coherence': 'To some extent', 'Tutor_Tone': 'Encouraging'}, 'annotation_point': {'Mistake_Identification': 2, 'Mistake_Location': 3, 'Revealing_of_the_Answer': 3, 'Providing_Guidance': 2, 'Actionability': 2, 'humanlikeness': 1, 'Coherence': 2, 'Tutor_Tone': 1}}}}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tutor</th>\n",
       "      <th>Mistake_Identification</th>\n",
       "      <th>Mistake_Location</th>\n",
       "      <th>Revealing_of_the_Answer</th>\n",
       "      <th>Providing_Guidance</th>\n",
       "      <th>Actionability</th>\n",
       "      <th>Coherence</th>\n",
       "      <th>Tutor_Tone</th>\n",
       "      <th>Human-likeness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Expert</td>\n",
       "      <td>81.25</td>\n",
       "      <td>68.75</td>\n",
       "      <td>97.92</td>\n",
       "      <td>72.92</td>\n",
       "      <td>81.77</td>\n",
       "      <td>84.90</td>\n",
       "      <td>17.19</td>\n",
       "      <td>94.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Expert_paper</td>\n",
       "      <td>76.04</td>\n",
       "      <td>63.02</td>\n",
       "      <td>90.62</td>\n",
       "      <td>67.19</td>\n",
       "      <td>76.04</td>\n",
       "      <td>79.17</td>\n",
       "      <td>92.19</td>\n",
       "      <td>87.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>GPT-4_paper</td>\n",
       "      <td>94.27</td>\n",
       "      <td>84.38</td>\n",
       "      <td>53.12</td>\n",
       "      <td>76.04</td>\n",
       "      <td>46.35</td>\n",
       "      <td>90.17</td>\n",
       "      <td>37.50</td>\n",
       "      <td>89.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPT4</td>\n",
       "      <td>94.27</td>\n",
       "      <td>85.42</td>\n",
       "      <td>54.69</td>\n",
       "      <td>77.08</td>\n",
       "      <td>46.88</td>\n",
       "      <td>92.71</td>\n",
       "      <td>36.98</td>\n",
       "      <td>93.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gemini</td>\n",
       "      <td>87.50</td>\n",
       "      <td>62.50</td>\n",
       "      <td>92.71</td>\n",
       "      <td>58.85</td>\n",
       "      <td>61.98</td>\n",
       "      <td>82.29</td>\n",
       "      <td>39.58</td>\n",
       "      <td>95.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Gemini_paper</td>\n",
       "      <td>63.02</td>\n",
       "      <td>39.58</td>\n",
       "      <td>67.71</td>\n",
       "      <td>37.50</td>\n",
       "      <td>42.71</td>\n",
       "      <td>56.77</td>\n",
       "      <td>21.88</td>\n",
       "      <td>68.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Llama31405B</td>\n",
       "      <td>95.31</td>\n",
       "      <td>84.90</td>\n",
       "      <td>81.77</td>\n",
       "      <td>77.60</td>\n",
       "      <td>75.52</td>\n",
       "      <td>94.27</td>\n",
       "      <td>17.71</td>\n",
       "      <td>93.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Llama31405B_paper</td>\n",
       "      <td>94.27</td>\n",
       "      <td>84.38</td>\n",
       "      <td>80.73</td>\n",
       "      <td>77.08</td>\n",
       "      <td>74.48</td>\n",
       "      <td>91.67</td>\n",
       "      <td>16.15</td>\n",
       "      <td>90.62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama318B</td>\n",
       "      <td>81.25</td>\n",
       "      <td>56.25</td>\n",
       "      <td>76.56</td>\n",
       "      <td>46.88</td>\n",
       "      <td>42.71</td>\n",
       "      <td>82.81</td>\n",
       "      <td>19.79</td>\n",
       "      <td>96.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Llama318B_paper</td>\n",
       "      <td>80.21</td>\n",
       "      <td>54.69</td>\n",
       "      <td>73.96</td>\n",
       "      <td>45.31</td>\n",
       "      <td>42.71</td>\n",
       "      <td>80.73</td>\n",
       "      <td>19.79</td>\n",
       "      <td>93.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mistral</td>\n",
       "      <td>93.23</td>\n",
       "      <td>74.48</td>\n",
       "      <td>89.06</td>\n",
       "      <td>66.15</td>\n",
       "      <td>71.35</td>\n",
       "      <td>88.02</td>\n",
       "      <td>16.67</td>\n",
       "      <td>97.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Mistral_paper</td>\n",
       "      <td>93.23</td>\n",
       "      <td>73.44</td>\n",
       "      <td>86.46</td>\n",
       "      <td>63.54</td>\n",
       "      <td>70.31</td>\n",
       "      <td>86.98</td>\n",
       "      <td>15.10</td>\n",
       "      <td>95.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Novice</td>\n",
       "      <td>49.06</td>\n",
       "      <td>16.98</td>\n",
       "      <td>88.68</td>\n",
       "      <td>13.21</td>\n",
       "      <td>1.89</td>\n",
       "      <td>56.60</td>\n",
       "      <td>54.72</td>\n",
       "      <td>37.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Novice_paper</td>\n",
       "      <td>43.33</td>\n",
       "      <td>16.67</td>\n",
       "      <td>80.00</td>\n",
       "      <td>11.67</td>\n",
       "      <td>1.67</td>\n",
       "      <td>50.00</td>\n",
       "      <td>90.00</td>\n",
       "      <td>35.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Phi3</td>\n",
       "      <td>28.65</td>\n",
       "      <td>26.56</td>\n",
       "      <td>79.17</td>\n",
       "      <td>18.23</td>\n",
       "      <td>11.46</td>\n",
       "      <td>38.54</td>\n",
       "      <td>47.40</td>\n",
       "      <td>52.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Phi3_paper</td>\n",
       "      <td>28.65</td>\n",
       "      <td>26.04</td>\n",
       "      <td>73.96</td>\n",
       "      <td>17.71</td>\n",
       "      <td>11.98</td>\n",
       "      <td>39.58</td>\n",
       "      <td>45.31</td>\n",
       "      <td>52.08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sonnet</td>\n",
       "      <td>86.98</td>\n",
       "      <td>71.35</td>\n",
       "      <td>96.88</td>\n",
       "      <td>63.02</td>\n",
       "      <td>62.50</td>\n",
       "      <td>90.62</td>\n",
       "      <td>57.81</td>\n",
       "      <td>98.96</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sonnet_paper</td>\n",
       "      <td>85.42</td>\n",
       "      <td>69.79</td>\n",
       "      <td>94.79</td>\n",
       "      <td>59.38</td>\n",
       "      <td>60.94</td>\n",
       "      <td>88.54</td>\n",
       "      <td>54.69</td>\n",
       "      <td>96.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tutor  Mistake_Identification  Mistake_Location  \\\n",
       "5             Expert                   81.25             68.75   \n",
       "1       Expert_paper                   76.04             63.02   \n",
       "7        GPT-4_paper                   94.27             84.38   \n",
       "6               GPT4                   94.27             85.42   \n",
       "0             Gemini                   87.50             62.50   \n",
       "4       Gemini_paper                   63.02             39.58   \n",
       "3        Llama31405B                   95.31             84.90   \n",
       "8  Llama31405B_paper                   94.27             84.38   \n",
       "2          Llama318B                   81.25             56.25   \n",
       "2    Llama318B_paper                   80.21             54.69   \n",
       "4            Mistral                   93.23             74.48   \n",
       "6      Mistral_paper                   93.23             73.44   \n",
       "8             Novice                   49.06             16.98   \n",
       "0       Novice_paper                   43.33             16.67   \n",
       "1               Phi3                   28.65             26.56   \n",
       "3         Phi3_paper                   28.65             26.04   \n",
       "7             Sonnet                   86.98             71.35   \n",
       "5       Sonnet_paper                   85.42             69.79   \n",
       "\n",
       "   Revealing_of_the_Answer  Providing_Guidance  Actionability  Coherence  \\\n",
       "5                    97.92               72.92          81.77      84.90   \n",
       "1                    90.62               67.19          76.04      79.17   \n",
       "7                    53.12               76.04          46.35      90.17   \n",
       "6                    54.69               77.08          46.88      92.71   \n",
       "0                    92.71               58.85          61.98      82.29   \n",
       "4                    67.71               37.50          42.71      56.77   \n",
       "3                    81.77               77.60          75.52      94.27   \n",
       "8                    80.73               77.08          74.48      91.67   \n",
       "2                    76.56               46.88          42.71      82.81   \n",
       "2                    73.96               45.31          42.71      80.73   \n",
       "4                    89.06               66.15          71.35      88.02   \n",
       "6                    86.46               63.54          70.31      86.98   \n",
       "8                    88.68               13.21           1.89      56.60   \n",
       "0                    80.00               11.67           1.67      50.00   \n",
       "1                    79.17               18.23          11.46      38.54   \n",
       "3                    73.96               17.71          11.98      39.58   \n",
       "7                    96.88               63.02          62.50      90.62   \n",
       "5                    94.79               59.38          60.94      88.54   \n",
       "\n",
       "   Tutor_Tone  Human-likeness  \n",
       "5       17.19           94.79  \n",
       "1       92.19           87.50  \n",
       "7       37.50           89.62  \n",
       "6       36.98           93.23  \n",
       "0       39.58           95.31  \n",
       "4       21.88           68.23  \n",
       "3       17.71           93.23  \n",
       "8       16.15           90.62  \n",
       "2       19.79           96.35  \n",
       "2       19.79           93.75  \n",
       "4       16.67           97.40  \n",
       "6       15.10           95.31  \n",
       "8       54.72           37.74  \n",
       "0       90.00           35.00  \n",
       "1       47.40           52.08  \n",
       "3       45.31           52.08  \n",
       "7       57.81           98.96  \n",
       "5       54.69           96.35  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map the annotation label to the desiderata point\n",
    "MRBenchv1_data_mapped = []\n",
    "for data in MRBenchv1_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        annotation = value['annotation']\n",
    "        new_annotation = {}\n",
    "        for k, v in annotation.items():\n",
    "            new_annotation[k] = map_annotation_label(k,v)\n",
    "        value['annotation_point'] = new_annotation\n",
    "    MRBenchv1_data_mapped.append(data)\n",
    "print(MRBenchv1_data_mapped[0])\n",
    "# evaluate the desiderata point\n",
    "evaluation_result = evaluate_ordinary_desiderata(MRBenchv1_data)\n",
    "# print the evaluation result\n",
    "pd_result=print_evaluation_result(evaluation_result)\n",
    "\n",
    "import pandas as pd\n",
    "ss = pd.read_csv('../paper/paper_result.csv',sep='\\t')\n",
    "ss['Tutor'] = ss['Tutor'].apply(lambda x: x.replace(\"*\",\"\")+\"_paper\")\n",
    "concat_result = pd.concat([ss, pd_result], axis=0)\n",
    "concat_result = concat_result.sort_values(by='Tutor')\n",
    "concat_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa75bb74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6c913ee",
   "metadata": {},
   "source": [
    "# Calculte AC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "e108575e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "def pearson_corr(y_true, y_pred, method=\"pearson\"):\n",
    "    y_true = np.array(y_true, dtype=float)\n",
    "    y_pred = np.array(y_pred, dtype=float)\n",
    "    mask = ~np.isnan(y_true) & ~np.isnan(y_pred)\n",
    "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    if method == \"pearson\":\n",
    "        r = pearsonr(y_true, y_pred)\n",
    "    elif method == \"spearman\":\n",
    "        r= spearmanr(y_true, y_pred)\n",
    "    return round(r.correlation, 3), round(r.pvalue, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "074d06d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map the annotation label to the desiderata point\n",
    "from collections import defaultdict\n",
    "MRBenchv1_model_point = defaultdict(dict)\n",
    "for data in MRBenchv1_data:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        annotation = value['annotation']\n",
    "        # new_annotation = {}\n",
    "        for k, v in annotation.items():\n",
    "            annotation_point = map_annotation_label(k,v)\n",
    "            if k not in MRBenchv1_model_point[key]:\n",
    "                MRBenchv1_model_point[key][k] = []\n",
    "            MRBenchv1_model_point[key][k].append([data['conversation_id']+data['Split'], annotation_point])\n",
    "temp=\"/Users/mobvoi/Downloads/MRBench_V1_llama_eval.json\"\n",
    "from collections import defaultdict\n",
    "temp=json.load(open(temp))\n",
    "model_point = defaultdict(dict)\n",
    "for data in temp:\n",
    "    for key, value in data['anno_llm_responses'].items():\n",
    "        annotation_eval = value['annotation_eval']\n",
    "        for k, v in annotation_eval.items():\n",
    "            if k not in model_point[key]:\n",
    "                model_point[key][k] = []\n",
    "            model_point[key][k].append([data['conversation_id']+data['Split'], v['number']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "3a810170",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Mistake_Identification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Gemini</th>\n",
       "      <td>0.222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi3</th>\n",
       "      <td>0.632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama318B</th>\n",
       "      <td>0.083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Llama31405B</th>\n",
       "      <td>-0.033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral</th>\n",
       "      <td>0.263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Expert</th>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GPT4</th>\n",
       "      <td>0.360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sonnet</th>\n",
       "      <td>0.157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Novice</th>\n",
       "      <td>0.523</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Mistake_Identification\n",
       "Gemini                        0.222\n",
       "Phi3                          0.632\n",
       "Llama318B                     0.083\n",
       "Llama31405B                  -0.033\n",
       "Mistral                       0.263\n",
       "Expert                        0.176\n",
       "GPT4                          0.360\n",
       "Sonnet                        0.157\n",
       "Novice                        0.523"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mistake_Identification\n",
    "result= defaultdict(dict)\n",
    "for k, v in MRBenchv1_model_point.items():\n",
    "    for kk, vv in v.items():\n",
    "        if kk == \"Mistake_Identification\":\n",
    "            compare_model_result = model_point[k][kk.lower()]\n",
    "            group1 = pd.DataFrame(vv, columns=['conversation_id', 'annotation_point'])\n",
    "            group2 = pd.DataFrame(compare_model_result, columns=['conversation_id', 'annotation_point'])\n",
    "            merge_result = pd.merge(group1, group2, on='conversation_id', how='inner')\n",
    "            merge_result = merge_result.dropna().drop_duplicates(subset=['conversation_id'])\n",
    "            merge_result\n",
    "            y_true = merge_result['annotation_point_x']\n",
    "            y_pred = merge_result['annotation_point_y']\n",
    "            p_corr, p_pvalue = pearson_corr(y_true, y_pred, method=\"pearson\")\n",
    "            s_corr, s_pvalue = pearson_corr(y_true, y_pred, method=\"spearman\")\n",
    "            # result[kk][k] = [p_corr, p_pvalue, s_corr, s_pvalue, len(merge_result)]\n",
    "            result[kk][k] = p_corr\n",
    "            # break\n",
    "\n",
    "result = pd.DataFrame(result)\n",
    "result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fbe919",
   "metadata": {},
   "source": [
    "# Note\n",
    "DAMR: We can observe that there are differences in the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
